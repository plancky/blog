[{"content":"You are at your local cafe waiting for your possible soul-mate when suddenly buzz buzz, the phone vibrates, \u0026ldquo;I will be there soon! got stuck in traffic :(\u0026rdquo; the text reads. You unsurprisingly look at the wrist watch, it is 17mins past 6pm, the decided time, sigh\u0026hellip;. you sit back in your chair as you recall your prior dates,\n \u0026ldquo;KJ was 10mins late, Sarah 30, Linda 15, Anna 20, and yeah how could I forget Ella who was 1 hour late.\u0026rdquo; you said to yourself.\n You decide that you aren\u0026rsquo;t going to wait any longer than 25mins this time around, so intuitively, you calculate that the chance that you will leave alone just from your prior experiences is\n$$\\text{No. of dates who came past 25mins}\\over\\text{Total no. of dates that came late}$$\nwhich is $2/5$, or is it?\n The above example of probability in daily-life shows that we all have some notion of probability, The aim of this write-up is to formalise it mathematically by introducing necessary vocabulary, axioms and theorems.\n Uncertainity in determining the outcome of any process, termed as experiment, motivates us to analyse the experiment quantitatively by repeating the experiment over and over again to gather the list of all possible outcomes and determine numbers, called probabilities, for one outcome or a set of outcomes that tells us the chance, if the experiment was repeated in a similar setting, of that thing to appear again.\nVocabulary Let us break the above example down and build a common vocabulary to work with,\nWe call a single performance of an experiment a trial and each possible result an outcome. Comparing with the above example every date is a trial, no punn intended and all the delays in arrival \u0026ldquo;KJ was 10mins late, Astra 30, Skye 15 \u0026hellip;\u0026hellip;..\u0026rdquo; each one of these are outcomes.\nThe set ${10,30,15,20,60}$ is then called the sample space of the experiment. Thus, the sample space $S$ of the experiment is then the set of all possible outcomes of an individual trial.\nMost often, one is not interested in individual outcomes but in whether an outcome fits a certain description, say your date being late more than 25mins ${25,30,60}$; or more mathematically, belongs to a given subset A of the sample space S; Any event $A$ is a subset of the sample space $S$. $A \\subseteq S$\nAxioms and Theorems  More quantitatively, if an experiment has a total of $n_S$ outcomes in the sample space S, and $n_A$ of these outcomes correspond to the event $A$, then the probability that event $A$ will occur is $$P(A) = {n_A \\over n_S}$$$0 \\leq P(A) \\leq 1$, for any event $A$ in $S$. $P(S)=1$  If A and B are two events in S then,$$ P(A\\cup B) = P(A) + P(B) - P(A \\cap B)$$ if $A$ and $B$ are mutually exclusive, $A \\cap B = \\phi \\implies P(A\\cup B) = P(A) + P(B)$.   In the general case, When the probabilities are combined to calculate the probability for the union of the $n$ general events, the result, which may be proved by induction upon $n$, is $$P\\left(\\bigcup_i^nA_i \\right) = \\sum_i^nP(A_i) - \\sum_{i,j}P(A_i \\cap A_j) + \\sum_{i,j,k} P(A_i\\cap A_j\\cap A_k) -\\dots+(-1)^{n+1}P\\left(\\bigcap_i A_i\\right)$$\n Complement Law: We define complement $\\bar{A}$ of an event $A$ in $S$, st $\\bar{A} \\cup A = S$ and $\\bar{A}\\cap A = \\phi$ $$P(A) = 1 - P(\\bar{A})$$ $P(A \\cap B)$ where $A$ (say) is the union of a set of $n$ mutually exclusive events $A_i$,$$\\begin{aligned}A \\cap B \u0026amp;= \\bigcup_{i} A_i \\cap B \\\\P(A \\cap B) \u0026amp;=\\sum_{i} P(A_i \\cap B)\\end{aligned}$$\nIn the case where $A_i$ exhaust all possible outcomes, $A = S$  $$P(B) = \\sum_{i} P(A_i \\cap B)$$\nConditional Probability Conditional probability answers the question\n \u0026ldquo;What is the probability of occurence of an event A given that some other event B has occured?\u0026rdquo;\n $$\\begin{aligned}P(A \\cap B) \u0026amp;= P(A)P(B|A) \\\\ \u0026amp;= P(B)P(A|B) \\\\[0.5cm]P(A|B) \u0026amp;= {P(A\\cap B)\\over P(B)} \\\\ P(B|A) \u0026amp;= {P(A\\cap B)\\over P(A)}\\end{aligned}$$\nIn terms of Venn diagrams, we may think of $P(B|A)$ as the probability of $B$ in the reduced sample space defined by $A$; Introducing the condition of A occuring collapses the sample space to $A$, we then look for outcomes within $A$ that also belong to $B$ (that is $A \\cap B$).\nThus, if two events $A$ and $B$ are mutually exclusive then $$P(A|B)= 0 = P(B|A)$$\nAlso, if the occurence of one event, say A, does not affect the probability of occurence of the other, B, then the events are called independent events,\n$$P(B/A) = P(B) \\implies P(A\\cap B) = P(A)P(B)$$\nSummary Let us summarise this discussion with the example that we started with. Let $E$ be the event that $\\text{You leave alone}$ , we calculated $P(E) = 1/2$, the probability of you leaving alone, BUT we forgot to take into account one essential piece of information that should affect this probability,\n \u0026ldquo;You unsurprisingly look at the wrist watch, it is 17mins past 6pm\u0026rdquo;\n Your date is already 17mins late, so your sample space {10,30,15,20,60} has to be updated to {30,20,60}, therefore what we actually calculate is $P(E|A) = 1/3$, where A is the event that the date arrives after 17mins. Mathematically, this is stated as,\n$$\\begin{aligned} P(E|A) \u0026amp;= P(\\text{He will leave alone given that the date arrives after 17mins})\\\\[0.5cm] \u0026amp;= {P(\\text{He leaves alone and the date is 17mins late}) \\over P(\\text{The date arrives after 17mins })} = \\frac{1/5}{3/5} = \\frac{1}{3}\\end{aligned}$$\nFor diving into details of probability theory, I recommend \u0026ldquo;A First Course in Probability by Sheldon Ross\u0026rdquo;.\n","description":"Uncertainity in determining the outcome of any process, termed as **experiment**, motivates us to analyse the experiment quantitatively by repeating the experiment over and over again to gather the list of all possible outcomes and determine numbers, called **probabilities**, for one outcome or a set of outcomes that tells us the chance, if the experiment was repeated in a similar setting, of that thing to appear again.","id":0,"section":"posts","tags":["Probability Theory","Conditional probability","Mathematics","Computer Science"],"title":"Fundamentals of Probability Theory","uri":"http://plancky.github.io/blog/posts/fundamental_probability/"},{"content":"No pre-requisites required  Number System What do Roman numerals, Tally marks, Decimal and Binary number systems have in common? They are all used to describe Quantity or the number of things, each with its unique style motivated by a particular usecase.\nTally Marks exist from the primitive ages and were probably made to keep track of the stock of apples left and Roman numerals were invented to write relatively large quantities probably to ledger cash, it would not be really fun writing hundred or even a thousand in tally marks.\nDecimal system is the one with which you grew up with, atleast I did, which came to be after humanity invented 0 which comes really handy while simplifying fractions, it is also called the Base 10 system because we have 10 Basic states or configurations or symbols to work with, namely 0,1,2,3,4,5,6,7,8,9.\nThe decimal number system has two main attributes indices(place), base.\n In the Decimal system, a number is represented by a list of digits from 0 to 9, where each digit represents the coefficient for a power of 10 determined by it\u0026rsquo;s place.\n  Mathematically, if N is any number, its decimal equivalent $$x_b x_{b-1} \\cdots x_1 x_0 . x_{-1} \\cdots x_{a+1} x_{a}$$ has the relation,\n$$N = \\sum_{i=a}^b{x_i 10^i}$$\nwhere $x_i = {0,1,\\dots 9}$\n look at the following examples,\n$$114.5 = 1\\cdot10^2 + 1\\cdot10^1 + 4\\cdot 10^0 + 5\\cdot10^{-1}$$\n$$11.64 = 1\\cdot10^1 + 1\\cdot 10^0 + 6\\cdot10^{-1} + 4\\cdot10^{-2}$$\nNow, the binary (meaning \u0026ldquo;two\u0026rdquo;) number system is defined similarly but with a base of 2,\n In the Binary system, a number is represented by 0 or 1, where each digit represents the coefficient for a power of 2 for it\u0026rsquo;s place.\n  Mathematically, if N is a any number, its binary equivalent $$x_b x_{b-1} \\cdots x_1 x_0 . x_{-1} \\cdots x_{a+1} x_{a}$$ has the relation,\n$$ N = \\sum_{i=a}^b{x_i 2^i} $$\nwhere $x_i = {0,1}$\n $$ (1001)_2 = 1\\cdot2^3 + 0\\cdot 2^2 + 0\\cdot2^1 + 1\\cdot2^0 = 9 $$\n$$ (1001.01)_2 = 1\\cdot2^3 + 0\\cdot 2^2 + 0\\cdot2^1 + 1\\cdot2^0 + 0\\cdot2^{-1} + 1\\cdot2^{-2} = 9.25 $$\nThe Binary number system readily finds its way in the most useful Binary device (No pun intended) humans ever created, the transistor . A transistor can either be ON (if it outputs current) or OFF otherwise. Computers are built on these small devices and therfore to work with such a binary system, the binary number system seemed as the perfect choice until\u0026hellip;. we came short on the abilities of our memory devices.\nA bit alone can attain only two states (ON or OFF) and therefore it can only represent two possible values based on its state, conventionally 0 and 1, but a list of $n$ bits can collectively exist in $2^n$ unique combinations or states. This allows us to store $2^n$ different numbers in memory, each mapped directly from a unique combination of the list of stored bits.\n  Information is stored in computer memory as lists of bits of fixed length (8-bit, 64-bit), each list is known as a register, fundamentally storing values in variables comes down to filling these registers.\n  It is helpful to view number systems as a one-one linear mapping from the set of all possible states to the Real Number line. The idea being that if you have such a mapping at your disposal you can figure out the only unique real number corresponding to a given array of bits, which is what a computer needs to do in order to store numbers.\n   Fixed-point numbers In a fixed-point representation, we fix our imaginary decimal point somewhere in middle of our list of bits and store numbers accordingly.\nLet\u0026rsquo;s first look at the idea in base 10, given that we allocate last 3 places for the difits after the decimal, we store the decimal number $14.78$ as $14.78 \\times 10^3 = 14780$ and similarly\n   Decimal Fixed point representation     503.445 503445   2.34 2340   2300.3 2300300   23.3305 23330    any place after the 3rd place after the decimal is dropped.\nFor example, consider a 8-bit register where the last 3 bits are allotted to the value after the decimal point.\n$ {(18.625 \\times 2^{3} )}_{10} = (149)_{10}= (10010101)_2 $\nHere the number $18.625$ is stored in memory as\n   Binary Internal Fixed point representation     101.110 101110   101.11 101110   101.10 101100   101.1101 101110    Although it is easier to perform arithemetic with fixed point numbers, there are numerous occassions when you would want to deal with numbers of very small magnitude in the order of $10^{-12}$ and to be able to multiply and divide them by numbers of relatively large magnitude in the order of $10^{12}$ ; Now, If you wish to use fixed-point representation you would require roughly 80+ bits which might not seem alot in the modern age but 30 years ago they were very expensive, this forced computer scientists to look for better ways of storing decimal numbers.\nNegative fixed point numbers are generally stored here by alloting half of all possible states($2^n$) to negative numbers, they are obtained by taking the two\u0026rsquo;s complement, that is, flipping all the bits and adding one to the obtained binary,  There is not enough memory!! Information that you store anywhere takes up space, quite literally, made available to you by your storage device. Allocating 64-bit to a variable means that you are reserving $64$ bits for the value of that variable in the memory. You can represent $2^{64}$ or 18,446,744,073,709,551,616 numbers at maximum with 64 bits. If you decide to represent numbers using fixed-point representaion you will not be able to get a large range and high precision both, you have to trade one for another,\nsince one might have to deal with:-\n  Very large numbers like the speed of light $2.99,792,458 \\times 10^{8}$ in SI units\n  Very small numbers like the planck\u0026rsquo;s constant $6.62607004 \\times 10^{-34}$\n  Boring numbers like $9.86$\n  If your number system is a one-one maping from the set of all combinations/states of 64-bits to the co-domain of real numbers, this is a good time to ask which all real numbers do you actually want to map to?\nWhich all Numbers you might actually need ? If I gave you a million of your favourite fruits, will you mind if you lost 20 of them?\nSince a $1 \\times 10^6$ fruits minus 20 or even a 200 is still a million fruits, roughly speaking, you may not want to keep track of small differences when dealing with comparitively large numbers.\nSimilarly, when dealing with small quantities, say 5.89kg of fruits, every kilogram of fruit matters to you and so you can\u0026rsquo;t probably afford to lose the count of even 0.1kg.\nTo put it simply, error is best recognized when it is compared to the magnitude of the quantity in which error is being noticed. Such error is called relative error, reducing absolute error should always be our priority but for good approximations we try to keep the relative error as low as possible.\nOn the same train of thought, notice that you must have more configurations in your number system reprensenting small real numbers than larger ones. This idea was at the core of all numerical work we did and led to the creation of several different floating point representations for binary,\n so many infact that it became difficult for programmers to make design software with all these different floating point hardware units floating around.\nincluding the IEEE\u0026rsquo;s standard for floating-point arithmetic, the IEEE 754 which is a number system that became immesenly popular among computers after its establishment in 1985.\n Floating-point numbers (floats) Aha !! we already have an representation for to deal with scientific quantities,\nIt\u0026rsquo;s called the scientific notation and goes something like this,\n All decimal numbers $N$ can be written in the form,\n$$ N = m \\times 10^n $$\n where $n$ is called the exponent and helps \u0026ldquo;float\u0026rdquo; the decimal point around, $m$ is called the significand which is a nonzero decimal number such that $0\u0026lt;|m|\u0026lt;10$ and it simply ensures that each decimal number has only one scientific notation(one-one mapping), a notation with this feature is called a normalized notation.   As we pointed out in our discussion of scientific notation for decimal numbers, the decimal point is no longer fixed but its position is determined by another integer called the exponent. Let\u0026rsquo;s discuss more about floating point representation in binary by discussing the IEEE 754 standard for floating point arithemetic followed while designing all modern computers to handle float numbers.\nIEEE 754 In 1985, Institute of Electrical and Electronics Engineers or IEEE came up with a set of standard protocols for storing and performing arithemetic operations involving the class of floating-point numbers, these standards stand by the name IEEE 754 and was established to ease the burden on computer programmers.(since in the early days various different number systems were implemented by hardware manufacturers.)\nAnalogous to the scientific notation,\n Any IEEE 754 standard float gets stored in accordance with this notation, binary number B can be represented as,\n$$ B = (-1)^S (1.M)\\times 2^{E-E_b} $$\n where S is called the sign bit and determines the sign of $B$, we no longer use 2\u0026rsquo;s complement to find the additive inverse in this system, instead we just flip the sign bit. recall from earlier that (1.M) is nothing but the significand, since normalization demands that the leading digit is nonzero, we end up fixing a 1 there and therfore it need not be stored. If $(M)_{2}$ is called the mantissa and gets read as, \\[ (M)_{10} = \\sum_{i=-c}^{-1} m_i 2^i \\]\nwhere $c$ is the maximum number of bits $m_i$ allocated to M. The exponent $(E-E_b)_{10}$ does the job of moving the binary point of the significand around just like in the scientific notation. Here biased exponent $(E)_2$ gets stored in the memory and when being retrieved the bias $(E_b)_2$ is subracted from it to get the value of exponent.   IEEE standards are classified on the bases of number of bits that are allocated to store the float.\n   IEEE standard No. of bits in memory     half-precision or float16 1 sign bit + 5 bits exponent + 10 bits mantissa = 16   single-precision or float32 1 sign bit + 8 bits exponent + 23 bits mantissa = 32   double-precision or float64 1 sign bit + 11 bits exponent + 52 bits mantissa = 64    One more thing, some values of the biased exponent is reserved for special numbers like inf, 0 and nan as shown in the table down below. Also,\nwhen E is $(0\\dots0)_2$ the leading digit of significand is flipped from 1 to 0. Such a representation is called subnormal and IEEE 754 uses them when the value to be returned is smaller than the smallest possible normal float value (this situation is referred to as underflow).\nOn the other hand when the value to be returned is larger than the largest possible normal float value, inf is returned in accordance to the sign bit.\n   E M(base 10) S Value     $(1\\dots1)_2$ 0 0 inf (positive infinity)   $(1\\dots1)_2$ 0 1 -inf(negative infinity)   $(1\\dots1)_2$ non-zero 0,1 nan (not a number)   $(0\\dots0)_2$ 0 0,1 0   $(0\\dots0)_2$ non-zero 0,1 Subnormal number     NOTE : From the table you must have noticed that the number 0 has two representations, one for each sign bit, that is why you will see Python and other languages printing -0 during computations.\n The usable range and the machine epsilon Largest possible float64 value can be easily computed,\n$E$ gets stored in 11bits and therefore has $2^{11}$ states out of which $(00\\dots 0)_{2}$ and $(11\\dots1)_2$ are reserved, therefore $E_{max} = 2046$ and $E_{min} = 1$,\nThe bias $E_b$ can be calculated as $2^{11-1}-1 = 1023$.\nTherefore the unbiased exponent ranges between $1-1023 = -1022\\leq \\text{exponent} \\leq 2046-1023 = 1023$\n1 2 3 4 5  \u0026gt;\u0026gt;\u0026gt; import numpy as np \u0026gt;\u0026gt;\u0026gt; largest_positive = (1+np.sum(2**np.arange(-52,0,1,dtype= float)))*2**(2046-1023) \u0026gt;\u0026gt;\u0026gt; smallest_positive = (1+0)*2**(1-1023) \u0026gt;\u0026gt;\u0026gt; largest_positive,smallest_positive (1.7976931348623157e+308, 2.2250738585072014e-308)   The interval between these two numbers gives us the usable range on the positive side of the number line.\nEach float value is at a certain gap from it\u0026rsquo;s neighbouring values if we view the values on the real number line. That is what allows us to cover such a large range of values.\nIf $N_1$ is a float then it\u0026rsquo;s smallest increment $N_2$ can be produced by adding $(00\\dots01)_2 = 2^{-52}$ to the mantissa.\n$$ N_1 = (1.M_1) \\times 2^{E-E_b}$$\n$$ N_2 = (1.M_1 + 2^{-52}) \\times 2^{E-E_b}$$\n$$ \\epsilon(E-E_b) = N_2 - N_1 = 2^{-52} \\times 2^{E-E_b} = 2^{E-E_b-52} $$\n$$ \\text{Machine epsilon} = \\epsilon(1) = 2^{-52} = 2.220446049250313e-16 $$\nThe gap for any float value belonging different IEEE precisions can be obtained using numpy.spacing function.\n1 2 3 4 5 6 7 8 9  \u0026gt;\u0026gt;\u0026gt; x = np.single(1) \u0026gt;\u0026gt;\u0026gt; np.spacing(x) 1.1920929e-07 \u0026gt;\u0026gt;\u0026gt; y = np.double(1) \u0026gt;\u0026gt;\u0026gt; np.spacing(y) 2.220446049250313e-16 \u0026gt;\u0026gt;\u0026gt; z = np.half(1) \u0026gt;\u0026gt;\u0026gt; np.spacing(z) 0.000977   Although $\\epsilon(E-E_b)$ which represents the gap varies with the exponent, the relative error is roughly equal to the machine epsilon . This constant is useful as it represents the relative error in every real value represented using inexact floats.\n You can directly get the information about the float using,\n 1 2 3  \u0026gt;\u0026gt;\u0026gt; import sys \u0026gt;\u0026gt;\u0026gt; sys.float_info sys.float_info(max=1.7976931348623157e+308, max_exp=1024, max_10_exp=308, min=2.2250738585072014e-308, min_exp=-1021, min_10_exp=-307, dig=15, mant_dig=53, epsilon=2.220446049250313e-16, radix=2, rounds=1)   Rounding Real numbers which cannot be representated exactly by the IEEE 754 standard lie between two exactly representable values and therefore end up getting rounded while being stored, IEEE 754 recommends use of one of the following methods for rounding such numbers:-\n  Round to nearest :\nThe system chooses the nearer of the two possible neighbourhood values. If the exact value answer is exactly halfway between the two, the system chooses to store the one where the least significant bit of mantissa is zero. This behavior (round-to-even) prevents various undesirable effects.\nThis is the default rounding rule followed by python float and numpy double datatypes.\n1 2 3 4 5 6  \u0026gt;\u0026gt;\u0026gt; 2.7 + np.spacing(2.7)*0.5 2.7 \u0026gt;\u0026gt;\u0026gt; 2.7 + np.spacing(2.7)*0.4 2.7 \u0026gt;\u0026gt;\u0026gt; 2.7 + np.spacing(2.7)*0.55 2.7000000000000006     Round up, or round toward plus infinity :\nThe system chooses the larger of the two possible values (that is, the one further from zero if they are positive, and the one closer to zero if they are negative).\n  Round down, or round toward minus infinity :\nThe system chooses the smaller of the two possible values (that is, the one closer to zero if they are positive, and the one further from zero if they are negative).\n  Round toward zero, or chop, or truncate :\nThe system chooses the value that is closer to zero, in all cases.\n  ","description":"What do Roman numerals, Tally marks, Decimal and Binary number systems have in common? They are all used to describe Quantity or the number of things, each with its unique style motivated by a particular usecase. ","id":1,"section":"posts","tags":["Science","float","Number system","Computer Science","Computational Physics"],"title":"Hurdles in computation requiring exact decimals : floating-point representation","uri":"http://plancky.github.io/blog/posts/floating_bits/"},{"content":"Hi!! I am a planck, nice to meet you!\nDo drop a mail here if you like what I post.\nStay happy and blessed!!\n","description":"Something about me.","id":2,"section":"","tags":null,"title":"About","uri":"http://plancky.github.io/blog/about/"}]